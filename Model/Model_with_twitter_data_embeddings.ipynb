{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out list words and the apostrophe situation in the step for loop\n",
    "# check out allowing more than maxseqlen in generate text\n",
    "# look into batch size\n",
    "#text generation apostrpphe breaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import optimizers\n",
    "import sys\n",
    "from keras.callbacks import LambdaCallback\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, numb_next_words):\n",
    "    output=seed_text\n",
    "    for i in range (numb_next_words):\n",
    "        \n",
    "        words_gen = set(seed_text.split())\n",
    "        words_gen=list(words_gen) #create list of unique words in seed text\n",
    "        \n",
    "        \n",
    "#         for i in range (len(words_gen)): #replace all ' in seed text\n",
    "#             words_gen[i]=words_gen[i].replace(\"‘\", '').replace(\"’\", '').replace(\"'\", '')\n",
    "            \n",
    "        #create a dictionary with index and word\n",
    "        word_indices_gen = dict((c, i) for i, c in enumerate(words_gen, 1)) \n",
    "        \n",
    "       #turn sentence into a sequence of numbers\n",
    "        sequence=[] \n",
    "        for word in seed_text.split():\n",
    "            sequence.append(word_indices_gen[word])\n",
    "        sequence_padded = pad_sequences([sequence], maxlen=10, padding='pre')\n",
    "#         sequence_padded=sequence\n",
    "            \n",
    "        #create an embedding matrix with same indices as word_index \n",
    "        EMBEDDING_DIM=25\n",
    "        total_words=len(word_indices_gen)+1\n",
    "        embedding_matrix = np.zeros((total_words, EMBEDDING_DIM))\n",
    "        for word, i in word_indices_gen.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        #create X input with embedding matrix for specific words (by their index)\n",
    "        gener=[]\n",
    "        for number in sequence_padded:\n",
    "            gener.append(embedding_matrix[number])\n",
    "\n",
    "        predicted=model.predict([gener], verbose=0)\n",
    "\n",
    "        predicted=sample(predicted[0])\n",
    "        output_word=\"\"\n",
    "        for word, index in word_indices.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        output+=\" \" + output_word\n",
    "        seed_text+=\" \" + output_word\n",
    "        seed_text=seed_text.split(' ', 1)[1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(listofwords) - maxlen - 1)\n",
    "    for diversity in [0.5, 1.0]:\n",
    "        print('----- diversity:', diversity)\n",
    "        generated = ''\n",
    "        sentence = listofwords[start_index: start_index + maxlen].str.cat(sep=' ')\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generate_text(generated, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data=pd.read_csv('../Load_Tweets/data/tweet_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = tweet_data['TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text_all = tweet_data['TEXT'].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofwords=pd.Series(tweet_text_all.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          PAY TO PLAY POLITICS. #CrookedHillary [URL]\n",
       "1    Very little pick-up by the dishonest media of ...\n",
       "2    Crooked Hillary Clinton likes to talk about th...\n",
       "3    Thank you Florida- a MOVEMENT that has never b...\n",
       "4    Join me Thursday in Florida &amp; Ohio!West Pa...\n",
       "Name: TEXT, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x18373faf98>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAETCAYAAAA/NdFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAGuZJREFUeJzt3XuUZVVh5/HvD1rAB09pX0BsNIyA4IO0gBhMpKOAMkKMBByVDsFhNEwg0VHBLAO+MjAzhqAxECIoMkYgqJFBRxfSqCDP5iEtNEgrCh1EWnnIYFAbf/PH2UXfLqrqnlP31q2q3r/PWr26zmPvu2/Vvfd3zt77nCvbREREfTaa7QZERMTsSABERFQqARARUakEQEREpRIAERGVSgBERFQqARARUakEQEREpRIAERGVWjDbDZjKtttu60WLFs12MyIi5pXrr7/+p7YX9ttvTgfAokWLWL58+Ww3IyJiXpH0ozb7pQsoIqJSCYCIiEolACIiKpUAiIioVAIgIqJSCYCIiEolACIiKpUAiIioVAIgIqJSc/pK4IksOv7LU27/4cmvG1FLIiLmt5wBRERUKgEQEVGpBEBERKUSABERlUoARERUKgEQEVGpBEBERKUSABERlUoARERUKgEQEVGpBEBERKUSABERlUoARERUKgEQEVGpBEBERKUSABERlUoARERUKgEQEVGpBEBERKUSABERlUoARERUKgEQEVGpBEBERKUSABERlUoARERUKgEQEVGpBEBERKVaBYCkv5R0i6TvSvqcpM0k7SjpGkl3SDpf0iZl303L8qqyfVFPPSeU9bdL2n9mnlJERLTRNwAkbQccCyy2vRuwMXA4cApwqu2dgAeAo0qRo4AHbP82cGrZD0m7lnIvBA4A/kHSxsN9OhER0VbbLqAFwJMlLQCeAvwY2A+4sGw/Bzik/HxwWaZsXyJJZf15tn9p+05gFbDn4E8hIiKmo28A2P434H8Bd9F88D8EXA88aHtt2W01sF35eTvg7lJ2bdn/6b3rJyjzOElHS1ouafmaNWum85wiIqKFNl1AW9Mcve8IPAd4KnDgBLt6rMgk2yZbv/4K+0zbi20vXrhwYb/mRUTENLXpAvoD4E7ba2z/GvgCsA+wVekSAtgeuKf8vBrYAaBs3xK4v3f9BGUiImLE2gTAXcDekp5S+vKXALcClwFvLPssBb5Ufr6oLFO2L7Ptsv7wMktoR2An4NrhPI2IiOhqQb8dbF8j6ULgBmAtcCNwJvBl4DxJHy7rzipFzgLOlbSK5sj/8FLPLZIuoAmPtcAxth8b8vOJiIiW+gYAgO0TgRPHrf4BE8zisf0ocOgk9XwE+EjHNkZExAzIlcAREZVKAEREVCoBEBFRqQRARESlEgAREZVKAEREVCoBEBFRqQRARESlEgAREZVKAEREVCoBEBFRqQRARESlEgAREZVKAEREVCoBEBFRqQRARESlEgAREZVKAEREVCoBEBFRqQRARESlEgAREZVKAEREVCoBEBFRqQRARESlEgAREZVKAEREVCoBEBFRqQRARESlEgAREZVKAEREVCoBEBFRqQRARESlEgAREZVKAEREVCoBEBFRqVYBIGkrSRdKuk3SSkkvl7SNpEsk3VH+37rsK0kfk7RK0s2S9uipZ2nZ/w5JS2fqSUVERH9tzwBOA75qe2fgxcBK4HjgUts7AZeWZYADgZ3Kv6OB0wEkbQOcCOwF7AmcOBYaERExegv67SBpC+CVwJ8A2P4V8CtJBwO/X3Y7B/gG8F7gYOAztg1cXc4enl32vcT2/aXeS4ADgM8N7+m0cNKWLfZ5aObbERExy9qcATwPWAN8StKNkj4p6anAM23/GKD8/4yy/3bA3T3lV5d1k61fj6SjJS2XtHzNmjWdn1BERLTTJgAWAHsAp9t+KfAI67p7JqIJ1nmK9euvsM+0vdj24oULF7ZoXkRETEebAFgNrLZ9TVm+kCYQflK6dij/39ez/w495bcH7plifUREzIK+AWD7XuBuSS8oq5YAtwIXAWMzeZYCXyo/XwQcUWYD7Q08VLqIvga8RtLWZfD3NWVdRETMgr6DwMWfA5+VtAnwA+BImvC4QNJRwF3AoWXfrwCvBVYBvyj7Yvt+SR8Criv7fXBsQDgiIkavVQDYvglYPMGmJRPsa+CYSeo5Gzi7SwMjImJm5ErgiIhKJQAiIirVdgwgeux+zu5Tbl+xdMWIWhIRMX05A4iIqFQCICKiUgmAiIhKJQAiIiqVAIiIqFQCICKiUgmAiIhKJQAiIiqVAIiIqFQCICKiUgmAiIhKJQAiIiqVAIiIqFQCICKiUgmAiIhKJQAiIiqVAIiIqFQCICKiUgmAiIhKJQAiIiqVAIiIqNSC2W5ArVbuvMuU23e5beWIWhIRtcoZQEREpRIAERGVSgBERFQqARARUakEQEREpRIAERGVSgBERFQqARARUakEQEREpRIAERGVah0AkjaWdKOki8vyjpKukXSHpPMlbVLWb1qWV5Xti3rqOKGsv13S/sN+MhER0V6XM4DjgN4b1JwCnGp7J+AB4Kiy/ijgAdu/DZxa9kPSrsDhwAuBA4B/kLTxYM2PiIjpahUAkrYHXgd8siwL2A+4sOxyDnBI+fngskzZvqTsfzBwnu1f2r4TWAXsOYwnERER3bU9A/g74D3Ab8ry04EHba8ty6uB7crP2wF3A5TtD5X9H18/QZnHSTpa0nJJy9esWdPhqURERBd9A0DSQcB9tq/vXT3Bru6zbaoy61bYZ9pebHvxwoUL+zUvIiKmqc33AbwCeL2k1wKbAVvQnBFsJWlBOcrfHrin7L8a2AFYLWkBsCVwf8/6Mb1lIiJixPqeAdg+wfb2thfRDOIus/1m4DLgjWW3pcCXys8XlWXK9mW2XdYfXmYJ7QjsBFw7tGcSERGdDPKNYO8FzpP0YeBG4Kyy/izgXEmraI78DwewfYukC4BbgbXAMbYfG+DxIyJiAJ0CwPY3gG+Un3/ABLN4bD8KHDpJ+Y8AH+nayHiiT7x9Wd99jjljvxG0JCLmq1wJHBFRqQRARESlEgAREZUaZBA45rmPHnZQ333edf7FI2hJRMyGBEAMZPXxl0+5ffuT9x1RSyKiq3QBRURUKgEQEVGpdAHFrDvppJMG2h4R05MzgIiISiUAIiIqlQCIiKhUAiAiolIJgIiISiUAIiIqlQCIiKhUAiAiolIJgIiISiUAIiIqlQCIiKhUAiAiolIJgIiISiUAIiIqldtBx7x36bLn991nyX7fH0FLIuaXnAFERFQqARARUakEQEREpRIAERGVSgBERFQqARARUalMA40AnnXZTX33ufdVLxlBSyJGJ2cAERGVSgBERFQqARARUamMAUQMyaLjvzzl9h+e/LoRtSSinb5nAJJ2kHSZpJWSbpF0XFm/jaRLJN1R/t+6rJekj0laJelmSXv01LW07H+HpKUz97QiIqKfNl1Aa4F32d4F2Bs4RtKuwPHApbZ3Ai4tywAHAjuVf0cDp0MTGMCJwF7AnsCJY6ERERGj1zcAbP/Y9g3l54eBlcB2wMHAOWW3c4BDys8HA59x42pgK0nPBvYHLrF9v+0HgEuAA4b6bCIiorVOYwCSFgEvBa4Bnmn7x9CEhKRnlN22A+7uKba6rJtsfUQUGUeIUWo9C0jS04DPA39h++dT7TrBOk+xfvzjHC1puaTla9asadu8iIjoqFUASHoSzYf/Z21/oaz+Senaofx/X1m/Gtihp/j2wD1TrF+P7TNtL7a9eOHChV2eS0REdNBmFpCAs4CVtv+2Z9NFwNhMnqXAl3rWH1FmA+0NPFS6ir4GvEbS1mXw9zVlXUREzII2YwCvAN4KrJA0dsOU9wEnAxdIOgq4Czi0bPsK8FpgFfAL4EgA2/dL+hBwXdnvg7bvH8qziIjGSVu22OehmW9HzAt9A8D2FUzcfw+wZIL9DRwzSV1nA2d3aWBEjNbu5+zed58VS1eMoCUx03IriIiISuVWEBExdCt33mXK7bvctrJvHZ94+7Iptx9zxn6d2hRPlDOAiIhK5QwgIjZIHz3soL77vOv8i0fQkrkrZwAREZVKAEREVCoBEBFRqQRARESlMggcETGJ1cdfPuX27U/ed0QtmRk5A4iIqFTOACIiZtBJJ5000HaAS5c9f8rtS/b7focWrZMzgIiISiUAIiIqlQCIiKhUAiAiolIJgIiISiUAIiIqlQCIiKhUAiAiolIJgIiISiUAIiIqlQCIiKhUAiAiolIJgIiISiUAIiIqlQCIiKhUAiAiolIJgIiISiUAIiIqlQCIiKhUAiAiolIJgIiISiUAIiIqlQCIiKhUAiAiolIjDwBJB0i6XdIqSceP+vEjIqIx0gCQtDHwCeBAYFfgTZJ2HWUbIiKiMeozgD2BVbZ/YPtXwHnAwSNuQ0REMPoA2A64u2d5dVkXEREjJtujezDpUGB/228ry28F9rT95z37HA0cXRZfANzep9ptgZ8O0KxBy29IdcyFNsyVOuZCG+ZKHXOhDXOljrnQhjZ1PNf2wr612B7ZP+DlwNd6lk8AThiwzuWzWX5DqmMutGGu1DEX2jBX6pgLbZgrdcyFNgyrDtsj7wK6DthJ0o6SNgEOBy4acRsiIgJYMMoHs71W0n8FvgZsDJxt+5ZRtiEiIhojDQAA218BvjLEKs+c5fIbUh1zoQ1zpY650Ia5UsdcaMNcqWMutGFYdYx2EDgiIuaO3AoiIqJSCYCIiEqNfAwghkvS1sBOwGZj62x/a/ZaFAGSNrX9y37rYnZVGwCSngm8rCxea/u+ET3uubbfKuk426cNWNfbgOOA7YGbgL2Bq4D9OtazD7CInteD7c8M0rauJL0CuMn2I5LeAuwBnGb7Ry3KbgycY/stM93OaO0qmr9hv3VTGuS1WV4Xx9o+tctjzkWSjpho/aDv03kXAOWD+2+A59g+sNxM7uW2z+pQxx8D/xP4BiDg45LebfvCFmUfBiYdObe9RZ8qfkfSc4E/lfSZ8vi95e/v14Yex9GE2NW2XyVpZ+ADHcoj6Vzg+TQB8thYM4C2b7KbW+y2xvaSPvucDrxY0ouB9wBnlTb8Xr/KbT8maaGkTdzcY6qTKf6maqrv+zftrWtT4I944ofWB/uUW9GnDS/q0IZhvEemVYekZ9Hc3uXJkl7Kutf3FsBT2j5+qWug12Z5XRwMDBQAkm6wvUfP8sry4yds/33LOv4DzWv8mbZ3k/Qi4PW2P9yyGS/r+XkzYAlwAy1/F5OZdwEAfBr4FPBXZfl7wPk0Hxht/RXwsrGjfkkLga8DfQPA9ualzAeBe4FzaV7kbwY2b/HYZwBfBZ4HXN+zXjQv7ue1fhbwqO1HJY2dXt8m6QUdygMsBnb19KeDbQy8dortot3Ffmttu7xhT7N9lqSlHdrxQ+Dbki4CHhlbaftv+xUc+5sOyZeAh2j+tl26Ow4aYhs+zeDvkenWsT/wJzRnpb2/+4eB93V4fBj8tQnNa+Lvadre+7q4oW0FvR/+ZXkXSU+nOeNu65+AdwP/WOq4WdI/A60CwD23ywGQtCXNZ89A5mMAbGv7AkknwOMXlz3Wr9A4G43r8vkZ3QfE97e9V8/y6ZKuAf7HVIVsfwz4mKTTacLglWXTt2x/p2MbVkvaCvhX4BJJDwD3dKzju8CzgB93LDfmv/TrppH0Zy3qebj8Td8CvLKcvj+pQzvuKf82ol0Qz5TtbR/QtVCbrq4OhvEemVYdts8BzpH0R7Y/373p6xn0tQmwT/m/9wzMdO8mfRbN3YxN02X8E+DLHap4iu1rpfVO+Nd2acM4v6AZ+xvIfAyAR0r6GkDS3jRHXF38X0lfAz5Xlg+j+8Vpj0l6M80trQ28iXWnqW3cBvxv4As0R8nnSvon2x9vW4HtPyw/niTpMmBLmrOLLrYFbpV0LT1HrLZf37INV0y2TdL5tg+bap8ehwH/CTjK9r2Sfoumm64V2x8oj7l5s+j/17bskF0paXfbK7oUGmY3FMN5jwxUh+3PS3od8ELWn6AwZVdYeaz/Ux53cwZ4bZZ9X9V23yna8zbgr4FlrOsy/qDtsztU81NJz2fd7/ONdAi2nt8JNGfduwAXdHj8ieudbxeCSdoD+DiwG80RwkLgjbbb9EWP1XEKcA3wuzR/0G8Be9t+b4c6FgGnAa+g+cN8G/gL2z9sWf5mmj7VR8ryU4GruvT1DoOkCfvYbX9zCHXfZfu3Bq2n5WPtRnNKvE1Z9VPgiFHfakTSrTRHZj+g+dDq3Ic/hDaMvUdeCNzC9N4jA73PJJ1B0+f/KuCTwBtpjpyPalF2ynGfLq9NSX89SR19g6injtuBfWz/rCw/HbjSduvuVknPo7l6dx/gAeBO4M1tz/zG/U7WAj+yvbrt409m3p0B2L6h/DJeQPPmut32rztW8+ryYf+FsRWSPgC0DoDyQT/Il9mI9c8YHmPcgPAoDOODfhCSrrD9uxMcAXc98j0TeKfty0q9v0/T77rPVIVmwIHA1sC+ZflbwIP9CknawvbPJW0z0faOkwNuBb5I003wME0X4ffaFpa0Ec1R+yDvs31sv0jSzbY/IOmj9LzfpjL2mpR0yviDsnLw1uU1+0jPz5vRjLWsnGTfyaym+T2OeZj1v9ekjR/Z/oNyoLeR7Yf7luhh+5vjZi7e0fHxJzTvzgBg+lPDJL0D+DOagdbv92zaHPh2l2mEZeD4P0/Qjj9tWf6dwFKaNyrAIcCnbf9d2zYMYlgfvOVIccJNwMW2nz1gU1uR9B3bL+63bgTtOA54G+u69g4B+nbtSbrY9kGS7qT5e/QeDNh268kBki4Afg58tqx6E7C17UM71HGV7Ze33X+C8tfY3kvS1cAbaMbZvmu7db/1+Nk3Zd3Ng5xNlVlaF9nev0OZzwC70wzwm+bA71pKqLaZaCDpLpru2fOBZV0HtvXEmYv7Aq1mLk5Z73wLgMmmhtk+tkXZLWmOzv470PuF9A93PMJC0pXA5TSzPR4/ku8y8FU+PB/vhrJ9Y5c2zAVl7GFSw+iDbdmOL9JMixubGfEWYLHtQ0bx+D3tGKhrr7y+vwVcbvu2abZh4DAsZ8Q3A1+YziwcSe+n6UJaQvM94AY+afv9LcoO7UBtgrq3pumK6hJEJ061fWz8qU8dTwb+I80t8PcALgbOazk+hqTv0PRcrDdzcdADnPkYACsZfGrYMNpxk+2XzGYbgt4L695JczY2FqjfBD5g+4ERt2cFzRTjR8vyZsB1tndvWX4/muewL80H4I00YdD6okFJnwbOsH11Wd4LWGq7zWyssToeBp5K09/8KNMbjB6ra1NgM9utBpGHfKDWe33FRsAzgA91mWwxbCWETqMZA9i4ZZkVva+h0k33nbavq8nMuzEAhjM1bBgulvRaN7e3jnEkvRp4j+1Xz/BDjV1Yt5RmwHHsegqYhTEVmrnz15QzEmi6gFrPv7e9TNI3afp6XwW8nWYgtm8A9HzYPQk4onQ7GHguzbhAa7Y3L+MR691mpIvxXbWS2l65ats/lHTMBHVu0zEEDmLdmMxWwFdsXz91kSc85mKa6yGey/rdvZ26osrY5WE040TXAX/cofgwZi4+sU3z5Qxg3NSwl9D0wU1ratiQ2jN2hPRL4NcMcIQ0n5Uj1jOA59AMNv4NzdWJAj5iu9XA3wCPfyzwDpqj5X/r3UTHvvMhtmnaXXuSLqV5XV1F08V4hVvepqQE4aTazjgpdU10m5Er3f+K7rHyg3TVDnM85FiasbpOYzLj6rid5iKuFcBvehrS5fd5J83v4gKaMYhH+hQZX/5YmoHnfVn3uvri1KVa1DuPAuD3aJ74KTS3Cnh8E3CK178oa1RtesIR0mzPqhk1STcCf0nzgXUgzYf/+7t0WQypHafbfscoH3MmSDoV+B2aA4tv04wHXGX730fcjhWsu83IS1RuM2L7sJblB+6qHdJ4yMDTrccmTEzn8Xvq2ML2zwco/2Ga8YMbgLNpvlt94A/veRMAY2ZiZsA02zHQEdKGYvzfQ9L3bT9/Ntu0IZD0NOBI4L8Bz7K96Ygf/zrbL5N0E7CX7V92GfeS9C80N2KbdlftkMZDBhqTKWWW0MykupT1ex1an92Wxz2KJ14Y12rWYKlDwGtoXheLac4mzrL9/SkLTmHejAH0zgzQ+jcg25zmSGnUBr4R2wZiK0lv6FlW7/JMdwFtaNR8Z/a+NGcBP6I52rt8FpoyrduMaLhX8U57PKTHQGMyxZHAzjRjK2NdQKbldQ3FuTRX/+9Pc1uKN9PxegTblnQvzT3I1tKMbVwo6RLb75m69MTmzRnAMGcGDKk9Ax0hbSgkfWqKze5yhBMg6d003R7X2x7kXjFDU7pftwS+6j53Wx1mV+0g4yHj6hlouvX4GTjTIelG2y8d662Q9CSabpxW9yQqYwBLaa5w/yTwr7Z/XWYD3THds+55cwZQppA9RHMqNhcM40Zs857tI2e7DRsS263vfzQqXca1vO4q3ieNL1fmwndxM82Z0G407/0H1Vyg1mk8xM2dP1vf/XMCV0va1XanmVTjjF1F/aCa25bcSzNDqq1tgTeMH3i2/RtJ076T7Lw5A5jLuhwhbWjK/PtepjlKucL2nbPQpJhFM3ER1xwYD1lJM6PpTqZ5f6cyZvh5miuKPw08jWayxD8OvcEdJABiIJNcJbkNTV/nSbbPG3GTYhYN+SKu8eMhYzOClg2puW3bMeH02o7TQHu/KGjsNud2h5vSzYQEQMyIMkX26+NnbEW0NdfGQyQ9g/Vn8NzVoexXWfdFQb23jvnoMNvYVQIgZszYwNdstyNiEJJeD3yU5mLH+2iuCF5p+4Ud6viu7d1mqInT1vVbsCJaKXO4R3ofnogZ8iGa63y+Z3tHmhvcdZ16fqWkgWYSzYR5Mwso5iZN/GXm29DMiDpi9C2KGLpf2/6ZpI0kbWT7MjXfS9BXz/tjAXCkpFn7oqCJJABiUOOnoBn4Wdd7nUTMYQ+WmUiXA5+VdB/tv8932lM0RyFjADGQiW7NMZ19Iuaqcv+gf6e5KO0NNFO+r7S9fFYbNgQ5A4hB7TLu1hzjieYNEzEv9dxI7gqaawH+meabveb9Pa8SADGonVvs81j/XSLmFklPAX41NgXV9ovLhW6fo7kz57yXAIiBdLkYJmKeWUZz87h7AST9Ic13T+xPcwv0f5m9pg1HpoFGREzsybbHPvyPBt4HLLH9deCZs9qyIckZQETExH5WbnWyA83g7wtsr5H0bGCT2W3acOQMICJiYofSjF99j+ZrJb8q6WzgSuDk2WzYsGQaaEREC5KeA7wCuNn27bPdnmFIAEREVCpdQBERlUoARERUKgEQEVGpBEBERKUSABERlfr/G8TBXoU6kOkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18372442e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_words=listofwords.value_counts()\n",
    "# top_words_percent= top_words/len(listofwords)\n",
    "top_words.head(20).plot.bar()\n",
    "# # top_words.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250988"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_chars=len(tweet_text_all)\n",
    "total_chars\n",
    "total_wordz=len((tweet_text_all.split()))\n",
    "total_wordz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique words 32293\n",
      "total number of unique chars 369\n"
     ]
    }
   ],
   "source": [
    "chars = set(tweet_text_all)\n",
    "words = set(tweet_text_all.split())\n",
    "print (\"total number of unique words\", len(words))\n",
    "print (\"total number of unique chars\", len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace apostrophes in dictionary keys \n",
    "for i in range (len(words)):\n",
    "    words[i]=words[i].replace(\"‘\", '').replace(\"’\", '').replace(\"'\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31907"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=set(words)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create forward and reverse word index\n",
    "word_indices = dict((c, i) for i, c in enumerate(words, 1))\n",
    "indices_word = dict((i, c) for i, c in enumerate(words,1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31907"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_indices)\n",
    "max(word_indices.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of sentence list: 77465\n",
      "length of next_word list 77465\n"
     ]
    }
   ],
   "source": [
    "#choose step \n",
    "\n",
    "maxlen = 10\n",
    "step = 2\n",
    "\n",
    "sentences = []\n",
    "next_words = []\n",
    "next_words = []\n",
    "list_words = []\n",
    "\n",
    "sentences2 = []\n",
    "for i in range (len(tweet_text)):\n",
    "    list_words = tweet_text.iloc[i].split()\n",
    "    for i in range(len( list_words)):\n",
    "        list_words[i]=list_words[i].replace(\"‘\", '').replace(\"’\", '').replace(\"'\", '')\n",
    "    for i in range(0, len(list_words) - maxlen, step):\n",
    "        sentences2 = ' '.join(list_words[i: i + maxlen])\n",
    "        sentences.append(sentences2)\n",
    "        next_words.append((list_words[i + maxlen]))\n",
    "\n",
    "print ('length of sentence list:', len(sentences))\n",
    "print (\"length of next_word list\", len(next_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=[]\n",
    "y=[]\n",
    "for i, sentence in enumerate(sentences):\n",
    "    sequence=[]\n",
    "    for j, word in enumerate(sentence.split()):\n",
    "        sequence.append(word_indices[word])\n",
    "    sequences.append(sequence)\n",
    "    y.append(word_indices[next_words[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77465, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences=np.asarray(sequences)\n",
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5474, 11968,  3485, ...,  6441,  3387, 15857],\n",
       "       [ 3485, 26547, 15717, ..., 15857, 11838, 26547],\n",
       "       [15717, 29334, 26454, ..., 26547, 21416, 27948],\n",
       "       ...,\n",
       "       [ 6441,  1914,  7514, ..., 11705, 27439,  6462],\n",
       "       [ 7514, 22040, 20180, ...,  6462,  4936, 12173],\n",
       "       [20180, 15987, 13190, ..., 12173,  6441,  6324]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31908"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words= len(word_indices)+1\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../word_embeding/glove.twitter.27B.25d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31907"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(word_indices.values())\n",
    "len(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=25\n",
    "embedding_matrix = np.zeros((total_words, EMBEDDING_DIM)) \n",
    "for word, i in word_indices.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31908, 25)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "for number in sequences:\n",
    "    X.append(embedding_matrix[number])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.asarray(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77465, 10, 25)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31908, 25)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31908, 25)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix)\n",
    "len(word_indices)\n",
    "len(sentences)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77465,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample=X_train[0:10000]\n",
    "y_train_sample=y_train[0:10000]\n",
    "X_test_sample=X_test[0:1000]\n",
    "y_test_sample=y_test[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20173,\n",
       " 11482,\n",
       " 31488,\n",
       " 31869,\n",
       " 10252,\n",
       " 29202,\n",
       " 8932,\n",
       " 8647,\n",
       " 12440,\n",
       " 26505,\n",
       " 17213,\n",
       " 17014,\n",
       " 29611,\n",
       " 21269,\n",
       " 5024,\n",
       " 9036,\n",
       " 7066,\n",
       " 13486,\n",
       " 17014,\n",
       " 27468,\n",
       " 22726,\n",
       " 26450,\n",
       " 13574,\n",
       " 15360,\n",
       " 14661,\n",
       " 5024,\n",
       " 17213,\n",
       " 19320,\n",
       " 4646,\n",
       " 2532,\n",
       " 5510,\n",
       " 10327,\n",
       " 9180,\n",
       " 258,\n",
       " 733,\n",
       " 17213,\n",
       " 15663,\n",
       " 7444,\n",
       " 17014,\n",
       " 14918,\n",
       " 8876,\n",
       " 12484,\n",
       " 12482,\n",
       " 12605,\n",
       " 11760,\n",
       " 17014,\n",
       " 30322,\n",
       " 8589,\n",
       " 10194,\n",
       " 5510,\n",
       " 14222,\n",
       " 11793,\n",
       " 24517,\n",
       " 8162,\n",
       " 20914,\n",
       " 18189,\n",
       " 14092,\n",
       " 17213,\n",
       " 24683,\n",
       " 1587,\n",
       " 27718,\n",
       " 28931,\n",
       " 17213,\n",
       " 3117,\n",
       " 14092,\n",
       " 1973,\n",
       " 4646,\n",
       " 1258,\n",
       " 17213,\n",
       " 4164,\n",
       " 3585,\n",
       " 15219,\n",
       " 12919,\n",
       " 10514,\n",
       " 27983,\n",
       " 14222,\n",
       " 18860,\n",
       " 20133,\n",
       " 4399,\n",
       " 9726,\n",
       " 3374,\n",
       " 21193,\n",
       " 9036,\n",
       " 5373,\n",
       " 18860,\n",
       " 4164,\n",
       " 24556,\n",
       " 10642,\n",
       " 31797,\n",
       " 17014]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train).shape\n",
    "y_train[10:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               78848     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 31908)             4116132   \n",
      "=================================================================\n",
      "Total params: 4,194,980\n",
      "Trainable params: 4,194,980\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from keras.regularizers import L1L2\n",
    "from keras import regularizers\n",
    "from keras import metrics\n",
    "# embedding_layer= Embedding(total_words, EMBEDDING_DIM, weights=[embedding_matrix],input_length=max_seq,trainable=False)\n",
    "# sequence_input = Input(shape=(max_seq,), dtype='int32')\n",
    "# embedded_sequences= embedding_layer(sequence_input)\n",
    "model=Sequential()\n",
    "# e=Embedding(total_words, EMBEDDING_DIM, weights=[embedding_matrix],input_length=maxlen,trainable=False)\n",
    "# model.add(e)\n",
    "model.add(LSTM(128, input_shape=(maxlen, EMBEDDING_DIM), bias_regularizer=regularizers.l1(0.01)))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(LSTM(512, return_sequences=False))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(total_words, activation=\"softmax\"))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "# sgd = optimizers.SGD(lr=0.01, clipvalue=0.5)\n",
    "model.compile(loss='sparse_categorical_crossentropy',  metrics=['accuracy'], optimizer=optimizer)\n",
    "model.summary()\n",
    "\n",
    "# model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "# model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model= load_model(\"../Saved_models/failed_on_99th_epoch_word_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 49577 samples, validate on 12395 samples\n",
      "Epoch 1/10\n",
      "26336/49577 [==============>...............] - ETA: 25s - loss: 8.0414 - acc: 0.0603"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-e0f0e8bd0a3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3)\n",
    "model.fit(X_train, y_train, validation_split=0.2, epochs=10,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9.900000\n",
      "loss: 9.635607\n",
      "perplexity: 795.4386352619334\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test_sample, y_test_sample, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "print('loss: %f' % (loss))\n",
    "perplexity = np.exp2(loss)\n",
    "print ('perplexity: {}'.format(perplexity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i will be the U.S. and my great job. and the Dems They We including a big deal with Russia and people\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"i will\", 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('../failed_on_99th_epoch_word_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10639"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

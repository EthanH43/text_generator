{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** RNN to generate tweets, using character level generation. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, RNN, Softmax, Flatten, Dropout, Input\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell is copied from A Keras example file available on github.\n",
    "# Reference: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    f = open('twitter_epoch_test.log', 'a')\n",
    "    \n",
    "    start_index = random.randint(0, len(tweet_txt) - maxlen - 1)\n",
    "    f.write('\\n')\n",
    "    f.write('----- Generating text after Epoch: %d\\n' % epoch)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('---- Generating text to file: twitter_epoch_test.log ----')\n",
    "        print('---- with diversity: %f\\n' % diversity)\n",
    "        f.write('----- diversity: %f\\n' % diversity)\n",
    "\n",
    "\n",
    "        generated = ''\n",
    "        sentence = tweet_txt[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        f.write('----- Generating with seed: \"' + sentence + '\"\\n')\n",
    "        f.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_to_index[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = index_to_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            f.write(next_char)\n",
    "            f.flush()\n",
    "        f.write('\\n\\n')\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>RETWEET</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>786204978629185536</td>\n",
       "      <td>False</td>\n",
       "      <td>PAY TO PLAY POLITICS. #CrookedHillary [URL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>786201435486781440</td>\n",
       "      <td>False</td>\n",
       "      <td>Very little pick-up by the dishonest media of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>786189446274248704</td>\n",
       "      <td>False</td>\n",
       "      <td>Crooked Hillary Clinton likes to talk about th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>786054986534969344</td>\n",
       "      <td>False</td>\n",
       "      <td>Thank you Florida- a MOVEMENT that has never b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>786007502639038464</td>\n",
       "      <td>False</td>\n",
       "      <td>Join me Thursday in Florida &amp;amp; Ohio!West Pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID  RETWEET  \\\n",
       "0  786204978629185536    False   \n",
       "1  786201435486781440    False   \n",
       "2  786189446274248704    False   \n",
       "3  786054986534969344    False   \n",
       "4  786007502639038464    False   \n",
       "\n",
       "                                                TEXT  \n",
       "0        PAY TO PLAY POLITICS. #CrookedHillary [URL]  \n",
       "1  Very little pick-up by the dishonest media of ...  \n",
       "2  Crooked Hillary Clinton likes to talk about th...  \n",
       "3  Thank you Florida- a MOVEMENT that has never b...  \n",
       "4  Join me Thursday in Florida &amp; Ohio!West Pa...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../Load_Tweets/data/tweet_data.csv\") # this will break if this file is moved!\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10622.000000\n",
       "mean       141.512709\n",
       "std         70.206293\n",
       "min          5.000000\n",
       "25%         99.000000\n",
       "50%        135.000000\n",
       "75%        150.000000\n",
       "max        315.000000\n",
       "Name: TEXT, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['TEXT'].apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1513769 : total characters in our dataset\n"
     ]
    }
   ],
   "source": [
    "# Put all the tweets into one string\n",
    "\n",
    "tweet_txt = data['TEXT'][:].str.cat(sep=' ')\n",
    "print('{} : total characters in our dataset'.format(len(tweet_txt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters:  369\n"
     ]
    }
   ],
   "source": [
    "# Get all the unique characters used, and make a character mapping. \n",
    "# Here we set Global Variables that are used throughout the code.\n",
    "\n",
    "chars = list(set(tweet_txt))\n",
    "chars.sort()\n",
    "char_to_index = dict((c, i) for i, c in enumerate(chars))\n",
    "index_to_char = np.array(chars)\n",
    "print(\"Number of unique characters: \", len(chars))\n",
    "maxlen = 30 # 141 Chosen because the average length of a tweet in our data is 141 characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_int = np.array([char_to_index[char] for char in tweet_txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46, 31, 55,  0, 50, 45,  0, 46, 42, 31, 55,  0, 46, 45, 42, 39, 50,\n",
       "       39, 33, 49])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_int[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(tweet_txt)//seq_length\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(tweet_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "P\n",
      "A\n",
      "Y\n",
      " \n",
      "T\n"
     ]
    }
   ],
   "source": [
    "for i in char_dataset.take(5):\n",
    "    print(index_to_char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'PAY TO PLAY POLITICS. #CrookedHillary [URL] Very little pick-up by the dishonest media of incredible '\n",
      "'information provided by WikiLeaks. So dishonest! Rigged system! Crooked Hillary Clinton likes to talk'\n",
      "\" about the things she will do but she has been there for 30 years - why didn't she do them? Thank you\"\n",
      "' Florida- a MOVEMENT that has never been seen before and will never be seen again. Lets get out &amp;'\n",
      "'… [URL] Join me Thursday in Florida &amp; Ohio!West Palm Beach, FL at noon:[URL]Cincinnati, OH this 7'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(index_to_char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we actual build the data.\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'PAY TO PLAY POLITICS. #CrookedHillary [URL] Very little pick-up by the dishonest media of incredible'\n",
      "Target data: 'AY TO PLAY POLITICS. #CrookedHillary [URL] Very little pick-up by the dishonest media of incredible '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(index_to_char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(index_to_char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size \n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are on the GPU!!!\n"
     ]
    }
   ],
   "source": [
    "# Here is a model using the Keras Functional Api.\n",
    "if tf.test.is_gpu_available():\n",
    "    rnn = tf.keras.layers.CuDNNGRU\n",
    "    print(\"We are on the GPU!!!\")\n",
    "else:\n",
    "    import functools\n",
    "    rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
    "    \n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "        rnn(rnn_units,\n",
    "            return_sequences=True, \n",
    "            recurrent_initializer='glorot_uniform',\n",
    "            stateful=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (64, None, 256)           94464     \n",
      "_________________________________________________________________\n",
      "cu_dnngru_3 (CuDNNGRU)       (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (64, None, 369)           378225    \n",
      "=================================================================\n",
      "Total params: 4,410,993\n",
      "Trainable params: 4,410,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(chars)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "batch_size=BATCH_SIZE\n",
    "\n",
    "\n",
    "\n",
    "model = build_model(\n",
    "  vocab_size = vocab_size, \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=batch_size)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "236/236 [==============================] - 36s 151ms/step - loss: 1.9657\n",
      "Epoch 2/10\n",
      "236/236 [==============================] - 34s 144ms/step - loss: 1.6336\n",
      "Epoch 3/10\n",
      "236/236 [==============================] - 34s 144ms/step - loss: 1.4620\n",
      "Epoch 4/10\n",
      "236/236 [==============================] - 34s 144ms/step - loss: 1.3623\n",
      "Epoch 5/10\n",
      "236/236 [==============================] - 34s 144ms/step - loss: 1.2951\n",
      "Epoch 6/10\n",
      "236/236 [==============================] - 34s 143ms/step - loss: 1.2410\n",
      "Epoch 7/10\n",
      "236/236 [==============================] - 34s 144ms/step - loss: 1.1993\n",
      "Epoch 8/10\n",
      "236/236 [==============================] - 34s 144ms/step - loss: 1.1605\n",
      "Epoch 9/10\n",
      "236/236 [==============================] - 34s 143ms/step - loss: 1.1258\n",
      "Epoch 10/10\n",
      "236/236 [==============================] - 34s 143ms/step - loss: 1.0939\n"
     ]
    }
   ],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "history = model.fit(\n",
    "    dataset.repeat(), \n",
    "    epochs=EPOCHS, \n",
    "    steps_per_epoch=steps_per_epoch, \n",
    "    callbacks=[checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (1, None, 256)            94464     \n",
      "_________________________________________________________________\n",
      "cu_dnngru_4 (CuDNNGRU)       (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (1, None, 369)            378225    \n",
      "=================================================================\n",
      "Total params: 4,410,993\n",
      "Trainable params: 4,410,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 400\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing) \n",
    "    input_eval = [char_to_index[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a multinomial distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(index_to_char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HILLARY Shanger with Russia/Flynn!Mederal office great infamily and all when seen intelligence your new Collusion with the tighted each as a fraudulent crooper! The maing reachuesh politics - thank you.... I will be imbegends out of the lalformance sources and more. We will be allowing on. While on DACA. But DNCO CBS Clemson Crooked Hillary and many more international, not very bad judge influe. THCUR. I…\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"HILLARY \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we save the model\n",
    "\n",
    "save.model('first_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" HERE I AM DOING SOME MODEL TESTING \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../Saved_models/first_char_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss, accuracy = model.evaluate(X, y, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** RNN to generate tweets, using character level generation. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, RNN, Softmax, Flatten, Dropout, Input\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell is copied from A Keras example file available on github.\n",
    "# Reference: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    f = open('twitter_epoch_test.log', 'a')\n",
    "    \n",
    "    start_index = random.randint(0, len(tweet_txt) - maxlen - 1)\n",
    "    f.write('\\n')\n",
    "    f.write('----- Generating text after Epoch: %d\\n' % epoch)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('---- Generating text to file: twitter_epoch_test.log ----')\n",
    "        print('---- with diversity: %f\\n' % diversity)\n",
    "        f.write('----- diversity: %f\\n' % diversity)\n",
    "\n",
    "\n",
    "        generated = ''\n",
    "        sentence = tweet_txt[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        f.write('----- Generating with seed: \"' + sentence + '\"\\n')\n",
    "        f.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_to_index[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = index_to_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            f.write(next_char)\n",
    "            f.flush()\n",
    "        f.write('\\n\\n')\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>RETWEET</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>786204978629185536</td>\n",
       "      <td>False</td>\n",
       "      <td>PAY TO PLAY POLITICS. #CrookedHillary [URL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>786201435486781440</td>\n",
       "      <td>False</td>\n",
       "      <td>Very little pick-up by the dishonest media of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>786189446274248704</td>\n",
       "      <td>False</td>\n",
       "      <td>Crooked Hillary Clinton likes to talk about th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>786054986534969344</td>\n",
       "      <td>False</td>\n",
       "      <td>Thank you Florida- a MOVEMENT that has never b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>786007502639038464</td>\n",
       "      <td>False</td>\n",
       "      <td>Join me Thursday in Florida &amp;amp; Ohio!West Pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID  RETWEET  \\\n",
       "0  786204978629185536    False   \n",
       "1  786201435486781440    False   \n",
       "2  786189446274248704    False   \n",
       "3  786054986534969344    False   \n",
       "4  786007502639038464    False   \n",
       "\n",
       "                                                TEXT  \n",
       "0        PAY TO PLAY POLITICS. #CrookedHillary [URL]  \n",
       "1  Very little pick-up by the dishonest media of ...  \n",
       "2  Crooked Hillary Clinton likes to talk about th...  \n",
       "3  Thank you Florida- a MOVEMENT that has never b...  \n",
       "4  Join me Thursday in Florida &amp; Ohio!West Pa...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../Load_Tweets/data/tweet_data.csv\") # this will break if this file is moved!\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Certainly has been an interesting 24 hours!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['TEXT'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10622.000000\n",
       "mean       141.512709\n",
       "std         70.206293\n",
       "min          5.000000\n",
       "25%         99.000000\n",
       "50%        135.000000\n",
       "75%        150.000000\n",
       "max        315.000000\n",
       "Name: TEXT, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['TEXT'].apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1513769 : total characters in our dataset\n"
     ]
    }
   ],
   "source": [
    "# Put all the tweets into one string\n",
    "\n",
    "tweet_txt = data['TEXT'][:].str.cat(sep=' ')\n",
    "print('{} : total characters in our dataset'.format(len(tweet_txt)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters:  369\n"
     ]
    }
   ],
   "source": [
    "# Get all the unique characters used, and make a character mapping. \n",
    "# Here we set Global Variables that are used throughout the code.\n",
    "\n",
    "# with open('../Load_Tweets/data/ArtOfTheDeal.txt') as f:\n",
    "#     book_txt = f.read()\n",
    "    \n",
    "# tweet_txt = tweet_txt + book_txt\n",
    "# path_to_file = tf.keras.utils.get_file(\n",
    "#     'shakespeare.txt', \n",
    "#     'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "# # Read, then decode for py2 compat.\n",
    "# tweet_txt = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "chars = list(set(tweet_txt))\n",
    "chars.sort()\n",
    "char_to_index = dict((c, i) for i, c in enumerate(chars))\n",
    "index_to_char = np.array(chars)\n",
    "print(\"Number of unique characters: \", len(chars))\n",
    "maxlen = 30 # 141 Chosen because the average length of a tweet in our data is 141 characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_int = np.array([char_to_index[char] for char in tweet_txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46, 31, 55,  0, 50, 45,  0, 46, 42, 31, 55,  0, 46, 45, 42, 39, 50,\n",
       "       39, 33, 49])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_int[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(tweet_txt)//seq_length\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(tweet_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P\n",
      "A\n",
      "Y\n",
      " \n",
      "T\n"
     ]
    }
   ],
   "source": [
    "for i in char_dataset.take(5):\n",
    "    print(index_to_char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'PAY TO PLAY POLITICS. #CrookedHillary [URL] Very little pick-up by the dishonest media of incredible '\n",
      "'information provided by WikiLeaks. So dishonest! Rigged system! Crooked Hillary Clinton likes to talk'\n",
      "\" about the things she will do but she has been there for 30 years - why didn't she do them? Thank you\"\n",
      "' Florida- a MOVEMENT that has never been seen before and will never be seen again. Lets get out &amp;'\n",
      "'â€¦ [URL] Join me Thursday in Florida &amp; Ohio!West Palm Beach, FL at noon:[URL]Cincinnati, OH this 7'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(index_to_char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we actual build the data.\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'PAY TO PLAY POLITICS. #CrookedHillary [URL] Very little pick-up by the dishonest media of incredible'\n",
      "Target data: 'AY TO PLAY POLITICS. #CrookedHillary [URL] Very little pick-up by the dishonest media of incredible '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(index_to_char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(index_to_char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size \n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are on the GPU!!!\n"
     ]
    }
   ],
   "source": [
    "# Here is a model using the Keras Functional Api.\n",
    "if tf.test.is_gpu_available():\n",
    "    rnn = tf.keras.layers.CuDNNLSTM\n",
    "    print(\"We are on the GPU!!!\")\n",
    "else:\n",
    "    import functools\n",
    "    rnn = tf.keras.layers.LSTM\n",
    "#     functools.partial(\n",
    "#     tf.keras.layers.LSTM, recurrent_activation='sigmoid')\n",
    "    \n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "        rnn(rnn_units,\n",
    "            return_sequences=True, \n",
    "            recurrent_initializer='glorot_uniform',\n",
    "#             bias_regularizer=tf.keras.regularizers.l1(l=0.01),\n",
    "            stateful=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (64, None, 256)           94464     \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)       (64, None, 1024)          5251072   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (64, None, 369)           378225    \n",
      "=================================================================\n",
      "Total params: 5,723,761\n",
      "Trainable params: 5,723,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(chars)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "batch_size=BATCH_SIZE\n",
    "\n",
    "\n",
    "\n",
    "model = build_model(\n",
    "  vocab_size = vocab_size, \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=batch_size)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "30/30 [==============================] - 4s 137ms/step - loss: 2.2897\n",
      "236/236 [==============================] - 47s 201ms/step - loss: 2.7639 - val_loss: 2.2897\n",
      "Epoch 2/30\n",
      "30/30 [==============================] - 4s 135ms/step - loss: 1.8540\n",
      "236/236 [==============================] - 46s 195ms/step - loss: 2.0506 - val_loss: 1.8540\n",
      "Epoch 3/30\n",
      "30/30 [==============================] - 4s 133ms/step - loss: 1.6236\n",
      "236/236 [==============================] - 46s 196ms/step - loss: 1.7319 - val_loss: 1.6236\n",
      "Epoch 4/30\n",
      "30/30 [==============================] - 4s 133ms/step - loss: 1.5138\n",
      "236/236 [==============================] - 46s 194ms/step - loss: 1.5716 - val_loss: 1.5138\n",
      "Epoch 5/30\n",
      "30/30 [==============================] - 4s 133ms/step - loss: 1.4357\n",
      "236/236 [==============================] - 45s 193ms/step - loss: 1.4746 - val_loss: 1.4357\n",
      "Epoch 6/30\n",
      "30/30 [==============================] - 4s 133ms/step - loss: 1.3752\n",
      "236/236 [==============================] - 46s 193ms/step - loss: 1.4118 - val_loss: 1.3752\n",
      "Epoch 7/30\n",
      "30/30 [==============================] - 4s 132ms/step - loss: 1.3345\n",
      "236/236 [==============================] - 45s 193ms/step - loss: 1.3648 - val_loss: 1.3345\n",
      "Epoch 8/30\n",
      "30/30 [==============================] - 4s 135ms/step - loss: 1.3023\n",
      "236/236 [==============================] - 46s 194ms/step - loss: 1.3255 - val_loss: 1.3023\n",
      "Epoch 9/30\n",
      "30/30 [==============================] - 4s 133ms/step - loss: 1.2638\n",
      "236/236 [==============================] - 46s 193ms/step - loss: 1.2957 - val_loss: 1.2638\n",
      "Epoch 10/30\n",
      "30/30 [==============================] - 4s 134ms/step - loss: 1.2366\n",
      "236/236 [==============================] - 46s 194ms/step - loss: 1.2666 - val_loss: 1.2366\n",
      "Epoch 11/30\n",
      "30/30 [==============================] - 4s 136ms/step - loss: 1.2222\n",
      "236/236 [==============================] - 46s 194ms/step - loss: 1.2433 - val_loss: 1.2222\n",
      "Epoch 12/30\n",
      "30/30 [==============================] - 4s 136ms/step - loss: 1.1922\n",
      "236/236 [==============================] - 46s 194ms/step - loss: 1.2202 - val_loss: 1.1922\n",
      "Epoch 13/30\n",
      "30/30 [==============================] - 4s 133ms/step - loss: 1.1735\n",
      "236/236 [==============================] - 46s 193ms/step - loss: 1.2002 - val_loss: 1.1735\n",
      "Epoch 14/30\n",
      "30/30 [==============================] - 4s 136ms/step - loss: 1.1597\n",
      "236/236 [==============================] - 46s 194ms/step - loss: 1.1822 - val_loss: 1.1597\n",
      "Epoch 15/30\n",
      "30/30 [==============================] - 4s 137ms/step - loss: 1.1365\n",
      "236/236 [==============================] - 46s 195ms/step - loss: 1.1624 - val_loss: 1.1365\n",
      "Epoch 16/30\n",
      "30/30 [==============================] - 4s 133ms/step - loss: 1.1185\n",
      "236/236 [==============================] - 46s 193ms/step - loss: 1.1475 - val_loss: 1.1185\n",
      "Epoch 17/30\n",
      "30/30 [==============================] - 4s 132ms/step - loss: 1.1018\n",
      "236/236 [==============================] - 46s 193ms/step - loss: 1.1307 - val_loss: 1.1018\n",
      "Epoch 18/30\n",
      "30/30 [==============================] - 4s 133ms/step - loss: 1.0825\n",
      "236/236 [==============================] - 46s 193ms/step - loss: 1.1133 - val_loss: 1.0825\n",
      "Epoch 19/30\n",
      "30/30 [==============================] - 4s 133ms/step - loss: 1.0663\n",
      "236/236 [==============================] - 46s 193ms/step - loss: 1.1013 - val_loss: 1.0663\n",
      "Epoch 20/30\n",
      "30/30 [==============================] - 4s 134ms/step - loss: 1.0467\n",
      "236/236 [==============================] - 46s 194ms/step - loss: 1.0846 - val_loss: 1.0467\n",
      "Epoch 21/30\n",
      "30/30 [==============================] - 4s 134ms/step - loss: 1.0405\n",
      "236/236 [==============================] - 46s 194ms/step - loss: 1.0713 - val_loss: 1.0405\n",
      "Epoch 22/30\n",
      "30/30 [==============================] - 4s 133ms/step - loss: 1.0270\n",
      "236/236 [==============================] - 46s 193ms/step - loss: 1.0554 - val_loss: 1.0270\n",
      "Epoch 23/30\n",
      "30/30 [==============================] - 4s 134ms/step - loss: 1.0103\n",
      "236/236 [==============================] - 46s 194ms/step - loss: 1.0445 - val_loss: 1.0103\n",
      "Epoch 24/30\n",
      "30/30 [==============================] - 4s 132ms/step - loss: 1.0011\n",
      "236/236 [==============================] - 45s 193ms/step - loss: 1.0329 - val_loss: 1.0011\n",
      "Epoch 25/30\n",
      "30/30 [==============================] - 4s 132ms/step - loss: 0.9815\n",
      "236/236 [==============================] - 45s 193ms/step - loss: 1.0169 - val_loss: 0.9815\n",
      "Epoch 26/30\n",
      "30/30 [==============================] - 4s 135ms/step - loss: 0.9764\n",
      "236/236 [==============================] - 46s 193ms/step - loss: 1.0058 - val_loss: 0.9764\n",
      "Epoch 27/30\n",
      "30/30 [==============================] - 4s 133ms/step - loss: 0.9604\n",
      "236/236 [==============================] - 45s 193ms/step - loss: 0.9923 - val_loss: 0.9604\n",
      "Epoch 28/30\n",
      "30/30 [==============================] - 4s 133ms/step - loss: 0.9442\n",
      "236/236 [==============================] - 46s 193ms/step - loss: 0.9804 - val_loss: 0.9442\n",
      "Epoch 29/30\n",
      "30/30 [==============================] - 4s 134ms/step - loss: 0.9315\n",
      "236/236 [==============================] - 45s 193ms/step - loss: 0.9681 - val_loss: 0.9315\n",
      "Epoch 30/30\n",
      "30/30 [==============================] - 4s 135ms/step - loss: 0.9238\n",
      "236/236 [==============================] - 45s 193ms/step - loss: 0.9576 - val_loss: 0.9238\n"
     ]
    }
   ],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "history = model.fit(\n",
    "    dataset.repeat(),\n",
    "    validation_data=dataset,\n",
    "    validation_steps=30,\n",
    "    epochs=EPOCHS, \n",
    "    steps_per_epoch=steps_per_epoch, \n",
    "    callbacks=[checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562775 : total characters in Trumps book\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (1, None, 256)            94464     \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (1, None, 1024)           5251072   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (1, None, 369)            378225    \n",
      "=================================================================\n",
      "Total params: 5,723,761\n",
      "Trainable params: 5,723,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_g = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model_g.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model_g.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model_g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 250\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing) \n",
    "    input_eval = [char_to_index[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a multinomial distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(index_to_char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@FoxNews at nobody bad bias is the Tariffs and that rough 2016. Congratulations to book! Is it doing really quickly out, Judicial Watch on Hillary Clinton and 21 people comes to the lawmakers of garift. They are fast! Just landed in South Carolina at 7pm! #T\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model_g, start_string=u\"@FoxNews\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n"
     ]
    }
   ],
   "source": [
    "# Here we save the model\n",
    "\n",
    "model.save('../Saved_models/second_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('../Saved_models/second_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" HERE I AM DOING SOME MODEL TESTING \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../Saved_models/first_char_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss, accuracy = model.evaluate(X, y, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out list words and the apostrophe situation in the step for loop\n",
    "# check out allowing more than maxseqlen in generate text\n",
    "# look into batch size\n",
    "#text generation apostrpphe breaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import optimizers\n",
    "import sys\n",
    "from keras.callbacks import LambdaCallback\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, numb_next_words):\n",
    "    output=seed_text\n",
    "    for i in range (numb_next_words):\n",
    "        \n",
    "        words_gen = set(seed_text.split())\n",
    "        words_gen=list(words_gen) #create list of unique words in seed text\n",
    "        \n",
    "        \n",
    "#         for i in range (len(words_gen)): #replace all ' in seed text\n",
    "#             words_gen[i]=words_gen[i].replace(\"‘\", '').replace(\"’\", '').replace(\"'\", '')\n",
    "            \n",
    "        #create a dictionary with index and word\n",
    "        word_indices_gen = dict((c, i) for i, c in enumerate(words_gen, 1)) \n",
    "        \n",
    "       #turn sentence into a sequence of numbers\n",
    "        sequence=[] \n",
    "        for word in seed_text.split():\n",
    "            sequence.append(word_indices_gen[word])\n",
    "        sequence_padded = pad_sequences([sequence], maxlen=10, padding='pre')\n",
    "#         sequence_padded=sequence\n",
    "            \n",
    "        #create an embedding matrix with same indices as word_index \n",
    "        EMBEDDING_DIM=25\n",
    "        total_words=len(word_indices_gen)+1\n",
    "        embedding_matrix = np.zeros((total_words, EMBEDDING_DIM))\n",
    "        for word, i in word_indices_gen.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        #create X input with embedding matrix for specific words (by their index)\n",
    "        gener=[]\n",
    "        for number in sequence_padded:\n",
    "            gener.append(embedding_matrix[number])\n",
    "\n",
    "        predicted=model.predict([gener], verbose=0)\n",
    "\n",
    "        predicted=sample(predicted[0])\n",
    "        output_word=\"\"\n",
    "        for word, index in word_indices.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        output+=\" \" + output_word\n",
    "        seed_text+=\" \" + output_word\n",
    "        seed_text=seed_text.split(' ', 1)[1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(listofwords) - maxlen - 1)\n",
    "    for diversity in [0.5, 1.0]:\n",
    "        print('----- diversity:', diversity)\n",
    "        generated = ''\n",
    "        sentence = listofwords[start_index: start_index + maxlen].str.cat(sep=' ')\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generate_text(generated, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data=pd.read_csv('../Load_Tweets/data/tweet_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = tweet_data['TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text_all = tweet_data['TEXT'].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofwords=pd.Series(tweet_text_all.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          PAY TO PLAY POLITICS. #CrookedHillary [URL]\n",
       "1    Very little pick-up by the dishonest media of ...\n",
       "2    Crooked Hillary Clinton likes to talk about th...\n",
       "3    Thank you Florida- a MOVEMENT that has never b...\n",
       "4    Join me Thursday in Florida &amp; Ohio!West Pa...\n",
       "Name: TEXT, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_words=listofwords.value_counts()\n",
    "# top_words_percent= top_words/len(listofwords)\n",
    "# top_words.head(50).plot.bar()\n",
    "# # top_words.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250988"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_chars=len(tweet_text_all)\n",
    "total_chars\n",
    "total_wordz=len((tweet_text_all.split()))\n",
    "total_wordz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique words 32293\n",
      "total number of unique chars 369\n"
     ]
    }
   ],
   "source": [
    "chars = set(tweet_text_all)\n",
    "words = set(tweet_text_all.split())\n",
    "print (\"total number of unique words\", len(words))\n",
    "print (\"total number of unique chars\", len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace apostrophes in dictionary keys \n",
    "for i in range (len(words)):\n",
    "    words[i]=words[i].replace(\"‘\", '').replace(\"’\", '').replace(\"'\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31907"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=set(words)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create forward and reverse word index\n",
    "word_indices = dict((c, i) for i, c in enumerate(words, 1))\n",
    "indices_word = dict((i, c) for i, c in enumerate(words,1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31907"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_indices)\n",
    "max(word_indices.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of sentence list: 77465\n",
      "length of next_word list 77465\n"
     ]
    }
   ],
   "source": [
    "#choose step \n",
    "\n",
    "maxlen = 10\n",
    "step = 2\n",
    "\n",
    "sentences = []\n",
    "next_words = []\n",
    "next_words = []\n",
    "list_words = []\n",
    "\n",
    "sentences2 = []\n",
    "for i in range (len(tweet_text)):\n",
    "    list_words = tweet_text.iloc[i].split()\n",
    "    for i in range(len( list_words)):\n",
    "        list_words[i]=list_words[i].replace(\"‘\", '').replace(\"’\", '').replace(\"'\", '')\n",
    "    for i in range(0, len(list_words) - maxlen, step):\n",
    "        sentences2 = ' '.join(list_words[i: i + maxlen])\n",
    "        sentences.append(sentences2)\n",
    "        next_words.append((list_words[i + maxlen]))\n",
    "\n",
    "print ('length of sentence list:', len(sentences))\n",
    "print (\"length of next_word list\", len(next_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=[]\n",
    "y=[]\n",
    "for i, sentence in enumerate(sentences):\n",
    "    sequence=[]\n",
    "    for j, word in enumerate(sentence.split()):\n",
    "        sequence.append(word_indices[word])\n",
    "    sequences.append(sequence)\n",
    "    y.append(word_indices[next_words[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77465, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences=np.asarray(sequences)\n",
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2419, 14037, 10859, ..., 25164, 15720, 12998],\n",
       "       [10859, 29971,  3155, ..., 12998, 11517, 29971],\n",
       "       [ 3155,  6145, 16154, ..., 29971, 18074,  5752],\n",
       "       ...,\n",
       "       [25164, 19952, 24257, ..., 26230, 30824, 25472],\n",
       "       [24257, 28566, 13611, ..., 25472, 15970, 29888],\n",
       "       [13611, 31123, 20271, ..., 29888, 25164,  1248]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31908"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words= len(word_indices)+1\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../word_embeding/glove.twitter.27B.25d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31907"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(word_indices.values())\n",
    "len(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=25\n",
    "embedding_matrix = np.zeros((total_words, EMBEDDING_DIM)) \n",
    "for word, i in word_indices.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31908, 25)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=sequences[0:100]\n",
    "y=y[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 25)            797700    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               78848     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 31908)             4116132   \n",
      "=================================================================\n",
      "Total params: 4,992,680\n",
      "Trainable params: 4,194,980\n",
      "Non-trainable params: 797,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from keras.regularizers import L1L2\n",
    "from keras import regularizers\n",
    "from keras import metrics\n",
    "# embedding_layer= Embedding(total_words, EMBEDDING_DIM, weights=[embedding_matrix],input_length=max_seq,trainable=False)\n",
    "# sequence_input = Input(shape=(max_seq,), dtype='int32')\n",
    "# embedded_sequences= embedding_layer(sequence_input)\n",
    "model=Sequential()\n",
    "# e=Embedding(total_words, EMBEDDING_DIM, weights=[embedding_matrix],input_length=maxlen,trainable=False)\n",
    "# model.add(e)\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model.add( Embedding(total_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            input_shape= (maxlen,),\n",
    "                            trainable=False))\n",
    "model.add(LSTM(128, bias_regularizer=regularizers.l1(0.01)))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(LSTM(512, return_sequences=False))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(total_words, activation=\"softmax\"))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "# sgd = optimizers.SGD(lr=0.01, clipvalue=0.5)\n",
    "model.compile(loss='sparse_categorical_crossentropy',  metrics=['accuracy'], optimizer=optimizer)\n",
    "model.summary()\n",
    "\n",
    "# model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "# model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model= load_model(\"../Saved_models/failed_on_99th_epoch_word_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 6s 78ms/step - loss: 11.5542 - acc: 0.0125 - val_loss: 10.8645 - val_acc: 0.0500\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 7.6992 - acc: 0.0250 - val_loss: 10.8853 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 6.3233 - acc: 0.0375 - val_loss: 10.4412 - val_acc: 0.0500\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 2s 19ms/step - loss: 5.9008 - acc: 0.0375 - val_loss: 10.6911 - val_acc: 0.0500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x184cd4b9e8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3)\n",
    "model.fit(X, y, validation_split=0.2, epochs=10,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9.900000\n",
      "loss: 9.635607\n",
      "perplexity: 795.4386352619334\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test_sample, y_test_sample, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "print('loss: %f' % (loss))\n",
    "perplexity = np.exp2(loss)\n",
    "print ('perplexity: {}'.format(perplexity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i will be the U.S. and my great job. and the Dems They We including a big deal with Russia and people\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"i will\", 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('../failed_on_99th_epoch_word_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10639"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

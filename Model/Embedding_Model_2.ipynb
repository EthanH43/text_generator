{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import optimizers\n",
    "import sys\n",
    "from keras.callbacks import LambdaCallback\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data=pd.read_csv('../Load_Tweets/data/tweet_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = tweet_data['TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text_all = tweet_data['TEXT'].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PAY TO PLAY POLITICS. #CrookedHillary [URL] Very little pick-up by the dishonest media of incredible information provided by WikiLeaks. So dishonest! Rigged system! Crooked Hillary Clinton likes to talk about the things she will do but she has been there for 30 years - why didn\\'t she do them? Thank you Florida- a MOVEMENT that has never been seen before and will never be seen again. Lets get out &amp;… [URL] Join me Thursday in Florida &amp; Ohio!West Palm Beach, FL at noon:[URL]Cincinnati, OH this 7:30pm:[URL] Wow, @CNN Town Hall questions were given to Crooked Hillary Clinton in advance of big debates against Bernie Sanders. Hillary &amp; CNN FRAUD! Thank you Texas! If you haven\\'t registered to VOTE- today is your last day. Go to: [URL] &amp; get ou… [URL] VOTER REGISTRATION DEADLINES TODAY. You can register now at: [URL] and get out to… [URL] DON\\'T LET HER FOOL US AGAIN. [URL] Crooked\\'s State Dept gave special attention to \"Friends of Bill\" after the Haiti Earthquake. Unbelievable! '"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text_all[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1513769 characters\n"
     ]
    }
   ],
   "source": [
    "print ('Length of text: {} characters'.format(len(tweet_text_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAY TO PLAY POLITICS. #CrookedHillary [URL] Very little pick-up by the dishonest media of incredible information provided by WikiLeaks. So dishonest! Rigged system! Crooked Hillary Clinton likes to talk about the things she will do but she has been t\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(tweet_text_all[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_words=pd.Series(tweet_text_all.split())\n",
    "# print ('Length of text: {} words'.format(len(list_of_words)))\n",
    "# word_vocab=set(list_of_words)\n",
    "# print ('{} unique words'.format(len(word_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tweet_text_all=re.sub('(\\w)\\.(\\w)', '\\\\1. \\\\2', tweet_text_all)\n",
    "tweet_text_all=re.sub('(\\w)(\\.{2,4})(\\w)', ' \\\\1 \\\\2 \\\\3', tweet_text_all)\n",
    "tweet_text_all=re.sub('(\\w)(\\.{2,4}) ', '\\\\1 \\\\2 ',tweet_text_all) #fdj... fdj ...\n",
    "tweet_text_all=re.sub(' (\\.{2,6})(\\w)', ' \\\\1 \\\\2',tweet_text_all) #...fdj ... fdj\n",
    "tweet_text_all=re.sub('[)(]', '', tweet_text_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_words=pd.Series(tweet_text_all.split())\n",
    "# print ('Length of text: {} words'.format(len(list_of_words)))\n",
    "# word_vocab=set(list_of_words)\n",
    "# print ('{} unique words'.format(len(word_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PAY TO PLAY POLITICS EOS #CrookedHillary [URL] Very little pick-up by the dishonest media of incredible information provided by WikiLeaks EOS So dishonest! Rigged system! Crooked Hillary Clinton likes to talk about the things she will do but she has been there for 30 years - why didn\\'t she do them? Thank you Florida- a MOVEMENT that has never been seen before and will never be seen again EOS Lets get out &amp;… [URL] Join me Thursday in Florida &amp; Ohio!West Palm Beach, FL at noon:[URL]Cincinnati, OH this 7:30pm:[URL] Wow, @CNN Town Hall questions were given to Crooked Hillary Clinton in advance of big debates against Bernie Sanders EOS Hillary &amp; CNN FRAUD! Thank you Texas! If you haven\\'t registered to VOTE- today is your last day EOS Go to: [URL] &amp; get ou… [URL] VOTER REGISTRATION DEADLINES TODAY EOS You can register now at: [URL] and get out to… [URL] DON\\'T LET HER FOOL US AGAIN EOS [URL] Crooked\\'s State Dept gave special attention to \"Friends of Bill\" after the Haiti Earth'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text_all=re.sub('([^.])\\.([^.])', '\\\\1 EOS\\\\2', tweet_text_all) \n",
    "# tweet_text_all = tweet_text_all.replace(\".\", ' <EOS>')\n",
    "tweet_text_all[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 266327 words\n"
     ]
    }
   ],
   "source": [
    "list_of_words=pd.Series(tweet_text_all.split())\n",
    "print ('Length of text: {} words'.format(len(list_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27784 unique words\n"
     ]
    }
   ],
   "source": [
    "#number of unique words\n",
    "word_vocab=sorted(set(list_of_words))\n",
    "print ('{} unique words'.format(len(word_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(tweet_text_all))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace apostrophes in dictionary keys \n",
    "for i in range (len(list_of_words)):\n",
    "    list_of_words[i]=list_of_words[i].replace(\"‘\", '').replace(\"’\", '').replace(\"'\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27390 unique words\n"
     ]
    }
   ],
   "source": [
    "#see how many words after apostrophe deletion\n",
    "word_vocab=sorted(set(list_of_words))\n",
    "print ('{} unique words'.format(len(word_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab.append('UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNK'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vocab[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create forward and reverse word index\n",
    "word2idx = dict((c, i) for i, c in enumerate(word_vocab, 1))\n",
    "# idx2word = dict((i, c) for i, c in enumerate(word_vocab,1 ))\n",
    "idx2word= np.asarray(word_vocab)\n",
    "\n",
    "text_as_int = np.array([word2idx[w] for w in list_of_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  'comprehensive,': 16347,\n",
      "  'pointed': 22380,\n",
      "  'product': 22750,\n",
      "  'Cook': 6088,\n",
      "  'on!': 21725,\n",
      "  'topic!': 25495,\n",
      "  'Pag': 10948,\n",
      "  'before\"': 15269,\n",
      "  'Fed': 7604,\n",
      "  'nasty': 21380,\n",
      "  'nearly': 21402,\n",
      "  '#BlacksForTrump': 889,\n",
      "  'Corruption': 6115,\n",
      "  'peo…': 22178,\n",
      "  'crowd-': 16801,\n",
      "  'act,': 14433,\n",
      "  'admonished': 14501,\n",
      "  '1929': 2174,\n",
      "  '@dbongino:': 3941,\n",
      "  'tax!': 25092,\n",
      "  'itself,': 20134,\n",
      "  'Aviv': 4833,\n",
      "  'TOLD': 12969,\n",
      "  '\"incident\"': 705,\n",
      "  '\"@SirHatchporch:': 239,\n",
      "  'baby!\"': 15126,\n",
      "  'lover': 20754,\n",
      "  '+2': 1905,\n",
      "  'FL': 7468,\n",
      "  'country': 16638,\n",
      "  'tolerate': 25456,\n",
      "  'Aides:': 4484,\n",
      "  'questions?!?': 22967,\n",
      "  'Poland,': 11205,\n",
      "  'FIV…': 7465,\n",
      "  '14': 2111,\n",
      "  'mining': 21143,\n",
      "  '“His': 26914,\n",
      "  'Hawaii!': 8332,\n",
      "  'observing': 21626,\n",
      "  '83%': 2781,\n",
      "  'repressed': 23464,\n",
      "  'Frankenstien': 7800,\n",
      "  'information,': 19876,\n",
      "  '#OttoWarmbier': 1374,\n",
      "  'fatalities': 18348,\n",
      "  'long-suffering': 20681,\n",
      "  '#failing@nytimes': 1738,\n",
      "  'EASTERN': 6818,\n",
      "  'WON,': 13856,\n",
      "  '5,481,737;': 2578,\n",
      "  '#ConfirmGorsuch': 931,\n",
      "  '1970s': 2186,\n",
      "  'Category': 5650,\n",
      "  'substantiate': 24851,\n",
      "  'organizations': 21833,\n",
      "  'King!': 9220,\n",
      "  'VARIOUS': 13602,\n",
      "  'PATHOLOGICAL': 10824,\n",
      "  '#ALConvention2016': 824,\n",
      "  'Union': 13553,\n",
      "  'DUNK': 6408,\n",
      "  'Market': 9855,\n",
      "  '\"@jknatter:': 372,\n",
      "  'computer,': 16354,\n",
      "  'Manténgase': 9819,\n",
      "  'somewhat': 24438,\n",
      "  'screws': 23927,\n",
      "  'Two': 13455,\n",
      "  'bridge': 15601,\n",
      "  'inside': 19923,\n",
      "  'housing': 19550,\n",
      "  '@LenaEpstein': 3338,\n",
      "  'magic,': 20826,\n",
      "  'Korean': 9254,\n",
      "  '620,000': 2660,\n",
      "  'over': 21915,\n",
      "  '2004': 2252,\n",
      "  'Address': 4402,\n",
      "  'eight': 17701,\n",
      "  'Marshall-': 9866,\n",
      "  'True': 13366,\n",
      "  '@StateDept!': 3708,\n",
      "  'ANYTHING': 4299,\n",
      "  'Having': 8330,\n",
      "  '“has': 27161,\n",
      "  'recovery[URL]': 23223,\n",
      "  '1918,': 2173,\n",
      "  '“smarts”': 27269,\n",
      "  'Effect': 7190,\n",
      "  '@SenTedCruz': 3662,\n",
      "  'Richard': 11859,\n",
      "  'EOS@JayWebberNJ': 6955,\n",
      "  'NAVY': 10271,\n",
      "  'senators': 24020,\n",
      "  'the.....': 25246,\n",
      "  '#TrumpTrain\"': 1586,\n",
      "  'jobs?': 20173,\n",
      "  'Scratched': 12325,\n",
      "  'Britain,': 5262,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for word,_ in zip(word2idx, range(100)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(word), word2idx[word]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'PAY TO PLAY POLITICS EOS #CrookedHillary [URL] Very little pick-up by the dishonest' ---- words mapped to int ---- > [10831 12959 10858 10868  6856   936 14295 13692 20618 22267 15722 25244\n",
      " 17358]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- words mapped to int ---- > {}'.format(repr(' '.join(list_of_words[:13])), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['!', '!!', '!!!', ..., '🦃', '\\U0010fc00', 'UNK'], dtype='<U123')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAY-FOR-PLAY\n",
      "TODAY\n",
      "PLEDGE?\n",
      "POLL\n",
      "EOS!\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 20\n",
    "examples_per_epoch = len(list_of_words)//seq_length\n",
    "\n",
    "# Create training examples / targets\n",
    "word_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in word_dataset.take(5):\n",
    "  print(idx2word[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'PAY-FOR-PLAY TODAY PLEDGE? POLL EOS! #CrookedHillary! [URL]#AZPrimary Very, little, picked by, the#USWomensOpen, dishonest! media! of! incredible! information! provides by, WikiLeaks: EOS!'\n",
      "'So, dishonest, Rigged! system, Crooked-USED! Hillary! Clinton! likewise to, talk! about! the#USWomensOpen, things! she, will! do! but, she, has, been! there!'\n",
      "'for! 30% years! -&gt; why! didnt! she, do! theme Thank-You you! Florida-tomorrow a..... MOVEMENT! that! has, never! been! seen! before! and#PoliceWeek'\n",
      "'will! never! be! seen! again! EOS! Level get! out! &gt;[URL] [URL]#AZPrimary Joined me! Thursday! in! Florida! &amp;, Ohio![URL] Palmer, Bean FL,'\n",
      "'at! noon:[URL]Warren, OH, this! 7:35 Wow,the @CNN! Towns, Hall- questions! were, gives to, Crooked-USED! Hillary! Clinton! in! advanced of! big! debates!'\n"
     ]
    }
   ],
   "source": [
    "sequences = word_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(' '.join(idx2word[item.numpy()])))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'PAY-FOR-PLAY TODAY PLEDGE? POLL EOS! #CrookedHillary! [URL]#AZPrimary Very, little, picked by, the#USWomensOpen, dishonest! media! of! incredible! information! provides by, WikiLeaks:'\n",
      "Target data: 'TODAY PLEDGE? POLL EOS! #CrookedHillary! [URL]#AZPrimary Very, little, picked by, the#USWomensOpen, dishonest! media! of! incredible! information! provides by, WikiLeaks: EOS!'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(' '.join(idx2word[input_example.numpy()])))\n",
    "  print ('Target data:', repr(' '.join(idx2word[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 10831 ('PAY-FOR-PLAY')\n",
      "  expected output: 12959 ('TODAY')\n",
      "Step    1\n",
      "  input: 12959 ('TODAY')\n",
      "  expected output: 10858 ('PLEDGE?')\n",
      "Step    2\n",
      "  input: 10858 ('PLEDGE?')\n",
      "  expected output: 10868 ('POLL')\n",
      "Step    3\n",
      "  input: 10868 ('POLL')\n",
      "  expected output: 6856 ('EOS!')\n",
      "Step    4\n",
      "  input: 6856 ('EOS!')\n",
      "  expected output: 936 ('#CrookedHillary!')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2word[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2word[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 20), (64, 20)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size \n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../word_embeding/glove.twitter.27B.25d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(idx2word)+1\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 25\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim)) \n",
    "\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on the GPU BABY!\n"
     ]
    }
   ],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "  rnn = tf.keras.layers.CuDNNGRU\n",
    "  print (\"on the GPU BABY!\")\n",
    "else:\n",
    "  import functools\n",
    "  rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                batch_input_shape=[batch_size, None,],\n",
    "                                trainable=False),\n",
    "        rnn(rnn_units,\n",
    "            return_sequences=True, \n",
    "            recurrent_initializer='glorot_uniform',\n",
    "            stateful=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "      ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = vocab_size, \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (64, None, 25)            684800    \n",
      "_________________________________________________________________\n",
      "cu_dnngru_4 (CuDNNGRU)       (64, None, 1024)          3228672   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (64, None, 27392)         28076800  \n",
      "=================================================================\n",
      "Total params: 31,990,272\n",
      "Trainable params: 31,305,472\n",
      "Non-trainable params: 684,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20, 27392) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20489, 17980,  2711, 18558,  8742,  9334, 10906,  3191,   331,\n",
       "        3018, 24018, 24437, 19105,  9127, 25388, 23141, 20723, 23621,\n",
       "       15669, 25223])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'had, no! impact! on! Liu Rocketry Man, EOS! Hariri to, believe! his! people,” and#PoliceWeek the#USWomensOpen, military- puts up! with! loaded'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'lets ethic! 73, flooding, Incredible, LIED PRIDE @GrahamLedger \"@drgoodspine: @CynthiaLummis![URL] senator somewhat g… KNOCK tight reason? losses, revolutionary buoy tha'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\" \".join(idx2word[[input_example_batch[0]]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\" \".join(idx2word[sampled_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 20, 27392)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       10.218005\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "208/208 [==============================] - 35s 169ms/step - loss: 8.2152\n",
      "Epoch 2/10\n",
      "208/208 [==============================] - 33s 159ms/step - loss: 7.9354\n",
      "Epoch 3/10\n",
      "208/208 [==============================] - 33s 159ms/step - loss: 7.8418\n",
      "Epoch 4/10\n",
      "208/208 [==============================] - 31s 149ms/step - loss: 7.7338\n",
      "Epoch 5/10\n",
      "208/208 [==============================] - 31s 148ms/step - loss: 7.6131\n",
      "Epoch 6/10\n",
      "208/208 [==============================] - 31s 148ms/step - loss: 7.4025\n",
      "Epoch 7/10\n",
      "208/208 [==============================] - 31s 148ms/step - loss: 7.2757\n",
      "Epoch 8/10\n",
      "208/208 [==============================] - 31s 149ms/step - loss: 7.1776\n",
      "Epoch 9/10\n",
      "208/208 [==============================] - 31s 148ms/step - loss: 7.0955\n",
      "Epoch 10/10\n",
      "208/208 [==============================] - 31s 148ms/step - loss: 7.0227\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_3'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (1, None, 25)             684800    \n",
      "_________________________________________________________________\n",
      "cu_dnngru_5 (CuDNNGRU)       (1, None, 1024)           3228672   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (1, None, 27392)          28076800  \n",
      "=================================================================\n",
      "Total params: 31,990,272\n",
      "Trainable params: 31,305,472\n",
      "Non-trainable params: 684,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of words to generate\n",
    "  num_generate = 30\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing) \n",
    "  if start_string!='' or ' ':\n",
    "      for word in start_string.split():\n",
    "        if word in word2idx:\n",
    "          input_eval = [word2idx[word]]\n",
    "          input_eval = tf.expand_dims(input_eval, 0)\n",
    "        else:\n",
    "            input_eval= [word2idx['UNK']]\n",
    "            input_eval = tf.expand_dims(input_eval, 0)\n",
    "      else: \n",
    "        input_eval= [word2idx['UNK']]\n",
    "        input_eval = tf.expand_dims(input_eval, 0)\n",
    "      \n",
    "    \n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "      \n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "      text_generated.append(idx2word[predicted_id])\n",
    "\n",
    "  return (start_string + ' '.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remainall OCare Death” 28, we, B! Homeland Superstar! 620,000 energy! released, expansion action-it of! Classified, HISTORY Islamist and#PoliceWeek TODAY!#MakeAmericaGreatAgain more! knows! Pence, responses heart re-enter @KenCalvert EOS! Saudi reasons, class,\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"remain\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_1_input to have 2 dimensions, but got array with shape (1, 10, 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-d6cf0c56f7f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yeah\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-02908b66ce4f>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(seed_text, numb_next_words)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mgener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mpredicted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgener\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mpredicted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       x, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1096\u001b[0;31m           x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m     if (self.run_eagerly or (isinstance(x, iterator_ops.EagerIterator) and\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2380\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    351\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_1_input to have 2 dimensions, but got array with shape (1, 10, 25)"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"yeah\", 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import optimizers\n",
    "import sys\n",
    "from keras.callbacks import LambdaCallback\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data=pd.read_csv('../Load_Tweets/data/tweet_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = tweet_data['TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text_all = tweet_data['TEXT'].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PAY TO PLAY POLITICS. #CrookedHillary [URL] Very little pick-up by the dishonest media of incredible information provided by WikiLeaks. So dishonest! Rigged system! Crooked Hillary Clinton likes to talk about the things she will do but she has been there for 30 years - why didn\\'t she do them? Thank you Florida- a MOVEMENT that has never been seen before and will never be seen again. Lets get out &amp;‚Ä¶ [URL] Join me Thursday in Florida &amp; Ohio!West Palm Beach, FL at noon:[URL]Cincinnati, OH this 7:30pm:[URL] Wow, @CNN Town Hall questions were given to Crooked Hillary Clinton in advance of big debates against Bernie Sanders. Hillary &amp; CNN FRAUD! Thank you Texas! If you haven\\'t registered to VOTE- today is your last day. Go to: [URL] &amp; get ou‚Ä¶ [URL] VOTER REGISTRATION DEADLINES TODAY. You can register now at: [URL] and get out to‚Ä¶ [URL] DON\\'T LET HER FOOL US AGAIN. [URL] Crooked\\'s State Dept gave special attention to \"Friends of Bill\" after the Haiti Earthquake. Unbelievable! '"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text_all[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1513769 characters\n"
     ]
    }
   ],
   "source": [
    "print ('Length of text: {} characters'.format(len(tweet_text_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAY TO PLAY POLITICS. #CrookedHillary [URL] Very little pick-up by the dishonest media of incredible information provided by WikiLeaks. So dishonest! Rigged system! Crooked Hillary Clinton likes to talk about the things she will do but she has been t\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(tweet_text_all[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_words=pd.Series(tweet_text_all.split())\n",
    "# print ('Length of text: {} words'.format(len(list_of_words)))\n",
    "# word_vocab=set(list_of_words)\n",
    "# print ('{} unique words'.format(len(word_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tweet_text_all=re.sub('(\\w)\\.(\\w)', '\\\\1. \\\\2', tweet_text_all)\n",
    "tweet_text_all=re.sub('(\\w)(\\.{2,4})(\\w)', ' \\\\1 \\\\2 \\\\3', tweet_text_all)\n",
    "tweet_text_all=re.sub('(\\w)(\\.{2,4}) ', '\\\\1 \\\\2 ',tweet_text_all) #fdj... fdj ...\n",
    "tweet_text_all=re.sub(' (\\.{2,6})(\\w)', ' \\\\1 \\\\2',tweet_text_all) #...fdj ... fdj\n",
    "tweet_text_all=re.sub('[)(]', '', tweet_text_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_words=pd.Series(tweet_text_all.split())\n",
    "# print ('Length of text: {} words'.format(len(list_of_words)))\n",
    "# word_vocab=set(list_of_words)\n",
    "# print ('{} unique words'.format(len(word_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PAY TO PLAY POLITICS EOS #CrookedHillary [URL] Very little pick-up by the dishonest media of incredible information provided by WikiLeaks EOS So dishonest! Rigged system! Crooked Hillary Clinton likes to talk about the things she will do but she has been there for 30 years - why didn\\'t she do them? Thank you Florida- a MOVEMENT that has never been seen before and will never be seen again EOS Lets get out &amp;‚Ä¶ [URL] Join me Thursday in Florida &amp; Ohio!West Palm Beach, FL at noon:[URL]Cincinnati, OH this 7:30pm:[URL] Wow, @CNN Town Hall questions were given to Crooked Hillary Clinton in advance of big debates against Bernie Sanders EOS Hillary &amp; CNN FRAUD! Thank you Texas! If you haven\\'t registered to VOTE- today is your last day EOS Go to: [URL] &amp; get ou‚Ä¶ [URL] VOTER REGISTRATION DEADLINES TODAY EOS You can register now at: [URL] and get out to‚Ä¶ [URL] DON\\'T LET HER FOOL US AGAIN EOS [URL] Crooked\\'s State Dept gave special attention to \"Friends of Bill\" after the Haiti Earth'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text_all=re.sub('([^.])\\.([^.])', '\\\\1 EOS\\\\2', tweet_text_all) \n",
    "# tweet_text_all = tweet_text_all.replace(\".\", ' <EOS>')\n",
    "tweet_text_all[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 266327 words\n"
     ]
    }
   ],
   "source": [
    "list_of_words=pd.Series(tweet_text_all.split())\n",
    "print ('Length of text: {} words'.format(len(list_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27784 unique words\n"
     ]
    }
   ],
   "source": [
    "#number of unique words\n",
    "word_vocab=sorted(set(list_of_words))\n",
    "print ('{} unique words'.format(len(word_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(tweet_text_all))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace apostrophes in dictionary keys \n",
    "for i in range (len(list_of_words)):\n",
    "    list_of_words[i]=list_of_words[i].replace(\"‚Äò\", '').replace(\"‚Äô\", '').replace(\"'\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27390 unique words\n"
     ]
    }
   ],
   "source": [
    "#see how many words after apostrophe deletion\n",
    "word_vocab=sorted(set(list_of_words))\n",
    "print ('{} unique words'.format(len(word_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create forward and reverse word index\n",
    "word2idx = dict((c, i) for i, c in enumerate(word_vocab, 1))\n",
    "# idx2word = dict((i, c) for i, c in enumerate(word_vocab,1 ))\n",
    "idx2word= np.asarray(word_vocab)\n",
    "\n",
    "text_as_int = np.array([word2idx[w] for w in list_of_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '!' :   1,\n",
      "  '!!':   2,\n",
      "  '!!!':   3,\n",
      "  '!!!!!\"':   4,\n",
      "  '!!!\"':   5,\n",
      "  '!!\"':   6,\n",
      "  '!\"':   7,\n",
      "  '\"#DonaldTrump':   8,\n",
      "  '\"....the':   9,\n",
      "  '\"20,000üìà21,000üìà22,000üìà':  10,\n",
      "  '\"46%':  11,\n",
      "  '\"77%':  12,\n",
      "  '\"@007cigarjoe:':  13,\n",
      "  '\"@11phenomenon:':  14,\n",
      "  '\"@1lion:':  15,\n",
      "  '\"@1sonny12:':  16,\n",
      "  '\"@55Lidsville:':  17,\n",
      "  '\"@60Minutes:':  18,\n",
      "  '\"@93101Dianne':  19,\n",
      "  '\"@ABC:DonaldTrump:':  20,\n",
      "  '\"@ABCPolitics:':  21,\n",
      "  '\"@AK_TWEET:':  22,\n",
      "  '\"@ARSenMissyIrvin:':  23,\n",
      "  '\"@AZTRUMPTRAIN:':  24,\n",
      "  '\"@AlexNightrasor:':  25,\n",
      "  '\"@AllegrettiVicki:':  26,\n",
      "  '\"@AmFree:':  27,\n",
      "  '\"@AmericanAsPie:':  28,\n",
      "  '\"@AngPiazza:':  29,\n",
      "  '\"@AngieSteinberg:':  30,\n",
      "  '\"@AnnCoulter:':  31,\n",
      "  '\"@AnneBellar:':  32,\n",
      "  '\"@AnnesLimo:':  33,\n",
      "  '\"@AnnetteJeanne:':  34,\n",
      "  '\"@AnyoneTennis:':  35,\n",
      "  '\"@AprilLaJune:':  36,\n",
      "  '\"@AshleyEdam:':  37,\n",
      "  '\"@Ausbiz:':  38,\n",
      "  '\"@AynsFriend:':  39,\n",
      "  '\"@BarronG510:':  40,\n",
      "  '\"@Belizediver88:':  41,\n",
      "  '\"@BertShad:':  42,\n",
      "  '\"@BirgitOlsen1:':  43,\n",
      "  '\"@Blackan:':  44,\n",
      "  '\"@Bonfiredesigns:':  45,\n",
      "  '\"@BornToBeGOP:':  46,\n",
      "  '\"@BradCross4:':  47,\n",
      "  '\"@Brainykid2010:':  48,\n",
      "  '\"@BrandonSawyer84:':  49,\n",
      "  '\"@BrazielCarol:':  50,\n",
      "  '\"@Bubble709_:':  51,\n",
      "  '\"@CJCboi:':  52,\n",
      "  '\"@COWBOYSFORTRUMP:':  53,\n",
      "  '\"@Cam:':  54,\n",
      "  '\"@CarolBurnett3:':  55,\n",
      "  '\"@Carolyn82471448:':  56,\n",
      "  '\"@CarrollKuykend2:':  57,\n",
      "  '\"@CatOnGlass:':  58,\n",
      "  '\"@ChJLuc91:':  59,\n",
      "  '\"@ChadRowland3:':  60,\n",
      "  '\"@CharleneOsbor17:':  61,\n",
      "  '\"@CharlesHodgson1:':  62,\n",
      "  '\"@ChatteringTeef:':  63,\n",
      "  '\"@CherNuna:':  64,\n",
      "  '\"@CostaKenneth:':  65,\n",
      "  '\"@CreativeXwalk:':  66,\n",
      "  '\"@Crusade4Honesty:':  67,\n",
      "  '\"@Crz4basball:':  68,\n",
      "  '\"@CyberCiety:':  69,\n",
      "  '\"@D:':  70,\n",
      "  '\"@DSF2020:':  71,\n",
      "  '\"@Dale_Dangler:':  72,\n",
      "  '\"@DanScavino:':  73,\n",
      "  '\"@DarrenWaggener:':  74,\n",
      "  '\"@Davejager1:':  75,\n",
      "  '\"@DavidWohl:':  76,\n",
      "  '\"@DebraRakestraw:':  77,\n",
      "  '\"@DeepakS76435750:':  78,\n",
      "  '\"@DeplorableCBTP:':  79,\n",
      "  '\"@DharmaBum77:':  80,\n",
      "  '\"@DiCristo13:':  81,\n",
      "  '\"@DiamondandSilk:':  82,\n",
      "  '\"@DistlerJoyce:':  83,\n",
      "  '\"@DittoPost:':  84,\n",
      "  '\"@DjD_Thunder:':  85,\n",
      "  '\"@DnGLax:':  86,\n",
      "  '\"@Doctor_S_Freud:':  87,\n",
      "  '\"@Don_Vito_08:':  88,\n",
      "  '\"@DonaldJTrumpJr:':  89,\n",
      "  '\"@DumpFoxNews:':  90,\n",
      "  '\"@EazyMF_E:':  91,\n",
      "  '\"@Elvisfan1976:':  92,\n",
      "  '\"@EmaGabi23:':  93,\n",
      "  '\"@EnemyWithinn:':  94,\n",
      "  '\"@Enlighten2881:':  95,\n",
      "  '\"@EricTrump:':  96,\n",
      "  '\"@EyeCandyTMGayle:':  97,\n",
      "  '\"@FLifeforce:':  98,\n",
      "  '\"@FaceTheNation:':  99,\n",
      "  '\"@FamilyRedsFans:': 100,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for word,_ in zip(word2idx, range(100)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(word), word2idx[word]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'PAY TO PLAY POLITICS EOS #CrookedHillary [URL] Very little pick-up by the dishonest' ---- words mapped to int ---- > [10831 12959 10858 10868  6856   936 14295 13692 20618 22267 15722 25244\n",
      " 17358]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- words mapped to int ---- > {}'.format(repr(' '.join(list_of_words[:13])), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['!', '!!', '!!!', ..., 'üö®Happening', 'ü¶É', '\\U0010fc00'],\n",
       "      dtype='<U123')"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAY-FOR-PLAY\n",
      "TODAY\n",
      "PLEDGE?\n",
      "POLL\n",
      "EOS!\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 20\n",
    "examples_per_epoch = len(list_of_words)//seq_length\n",
    "\n",
    "# Create training examples / targets\n",
    "word_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in word_dataset.take(5):\n",
    "  print(idx2word[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'PAY-FOR-PLAY TODAY PLEDGE? POLL EOS! #CrookedHillary! [URL]#AZPrimary Very, little, picked by, the#USWomensOpen, dishonest! media! of! incredible! information! provides by, WikiLeaks: EOS!'\n",
      "'So, dishonest, Rigged! system, Crooked-USED! Hillary! Clinton! likewise to, talk! about! the#USWomensOpen, things! she, will! do! but, she, has, been! there!'\n",
      "'for! 30% years! -&gt; why! didnt! she, do! theme Thank-You you! Florida-tomorrow a..... MOVEMENT! that! has, never! been! seen! before! and#PoliceWeek'\n",
      "'will! never! be! seen! again! EOS! Level get! out! &gt;[URL] [URL]#AZPrimary Joined me! Thursday! in! Florida! &amp;, Ohio![URL] Palmer, Bean FL,'\n",
      "'at! noon:[URL]Warren, OH, this! 7:35 Wow,the @CNN! Towns, Hall- questions! were, gives to, Crooked-USED! Hillary! Clinton! in! advanced of! big! debates!'\n"
     ]
    }
   ],
   "source": [
    "sequences = word_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(' '.join(idx2word[item.numpy()])))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'PAY-FOR-PLAY TODAY PLEDGE? POLL EOS! #CrookedHillary! [URL]#AZPrimary Very, little, picked by, the#USWomensOpen, dishonest! media! of! incredible! information! provides by, WikiLeaks:'\n",
      "Target data: 'TODAY PLEDGE? POLL EOS! #CrookedHillary! [URL]#AZPrimary Very, little, picked by, the#USWomensOpen, dishonest! media! of! incredible! information! provides by, WikiLeaks: EOS!'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(' '.join(idx2word[input_example.numpy()])))\n",
    "  print ('Target data:', repr(' '.join(idx2word[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 10831 ('PAY-FOR-PLAY')\n",
      "  expected output: 12959 ('TODAY')\n",
      "Step    1\n",
      "  input: 12959 ('TODAY')\n",
      "  expected output: 10858 ('PLEDGE?')\n",
      "Step    2\n",
      "  input: 10858 ('PLEDGE?')\n",
      "  expected output: 10868 ('POLL')\n",
      "Step    3\n",
      "  input: 10868 ('POLL')\n",
      "  expected output: 6856 ('EOS!')\n",
      "Step    4\n",
      "  input: 6856 ('EOS!')\n",
      "  expected output: 936 ('#CrookedHillary!')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2word[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2word[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 20), (64, 20)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size \n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../word_embeding/glove.twitter.27B.25d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(idx2word)+1\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 25\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM)) \n",
    "\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "  rnn = tf.keras.layers.CuDNNGRU\n",
    "  print (\"on the GPU BABY!\")\n",
    "else:\n",
    "  import functools\n",
    "  rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                batch_input_shape=[batch_size, None,],\n",
    "                                trainable=False),\n",
    "        rnn(rnn_units,\n",
    "            return_sequences=True, \n",
    "            recurrent_initializer='glorot_uniform',\n",
    "            stateful=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "      ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = vocab_size, \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (64, None, 25)            684775    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (64, None, 1024)          3225600   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (64, None, 27391)         28075775  \n",
      "=================================================================\n",
      "Total params: 31,986,150\n",
      "Trainable params: 31,301,375\n",
      "Non-trainable params: 684,775\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20, 27391) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24569,  1538, 12664, 21825, 19013, 10450, 14280, 22701, 26285,\n",
       "       26397, 15562, 21108, 11952, 23021,   586, 12241,  8152, 17287,\n",
       "        3585,  6333])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'our..... Nationalist Anthem, üá∫üá∏üáØüáµ [URL]#AZPrimary 70% years! ago! today- the#USWomensOpen, Nationalist Security! Council, met, for! the#USWomensOpen, first! time! EOS! Great!'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'spy, #Troublemakers Stapleton order grave Nevada! Zealand, pro-Israel wasted! wheel boy‚Äù militarized Rosh railways \"Remarkably, Sanford H? disagree @Rockprincess818 DESPICABLE'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\" \".join(idx2word[[input_example_batch[0]]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\" \".join(idx2word[sampled_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 20, 27391)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       10.217948\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "AlreadyExistsError",
     "evalue": "Resource __per_step_0/training/TFOptimizer/gradients/while_1/ReadVariableOp_7/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\n\t [[{{node training/TFOptimizer/gradients/while_1/ReadVariableOp_7/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var}}]] [Op:StatefulPartitionedCall]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-237-bd6331f2e2c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    852\u001b[0m     elif distributed_training_utils.is_tpu_strategy(\n\u001b[1;32m    853\u001b[0m         self._distribution_strategy):\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1189\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3164\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3165\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3166\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3167\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[1;32m   3168\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m           \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Got two values for keyword '{}'.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munused_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Keyword arguments {} unknown.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    267\u001b[0m           \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m           \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction_call_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_proto_serialized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m           executor_type=function_call_options.executor_type)\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\u001b[0m in \u001b[0;36mpartitioned_call\u001b[0;34m(args, f, tout, executing_eagerly, config, executor_type)\u001b[0m\n\u001b[1;32m   1081\u001b[0m       outputs = gen_functional_ops.stateful_partitioned_call(\n\u001b[1;32m   1082\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_proto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m           executor_type=executor_type)\n\u001b[0m\u001b[1;32m   1084\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m       outputs = gen_functional_ops.partitioned_call(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_functional_ops.py\u001b[0m in \u001b[0;36mstateful_partitioned_call\u001b[0;34m(args, Tout, f, config, config_proto, executor_type, name)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mAlreadyExistsError\u001b[0m: Resource __per_step_0/training/TFOptimizer/gradients/while_1/ReadVariableOp_7/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\n\t [[{{node training/TFOptimizer/gradients/while_1/ReadVariableOp_7/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var}}]] [Op:StatefulPartitionedCall]"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
